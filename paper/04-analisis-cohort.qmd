---
author: "Equipo EDUMER"
bibliography: "../input/bib/merit-factorial.bib"
csl: "../input/bib/apa6.csl"
---

## Cohort Invariance Test

```{r}
#| label: set
#| echo: false
#| message: false
#| warning: false

library(knitr)

knitr::opts_chunk$set(echo = F,
                      warning = F,
                      error = F, 
                      message = F) 

table_format <- if (is_html_output()) {
  "html"
} else if (is_latex_output()) {
  "latex"
}
table_format2 <- if (is_html_output()) {
  T
} else if (is_latex_output()) {
  F
}

options(kableExtra.html.bsTable = T)
options(knitr.kable.NA = "")

# funcion para exportar tablas en formato html o latex
```

```{r}
#| label: packages
#| include: false
#| echo: false

if (!require("pacman")) install.packages("pacman") #para instalar pacman en caso de no estar instalado

pacman::p_load(
  tidyverse,
  sjmisc,
  sjPlot,
  here,
  lavaan,
  psych,
  ggdist,
  semTools,
  gtools,
  kableExtra
)

options(scipen = 999) # para eliminar la notación científica
rm(list = ls()) # limpiar el ambiente 
```

```{r}
#| label: longitudinal data 
#| echo: false
#| output: false

load(file = here("output", "data", "db_long_proc.RData")) #cargar base con las dos olas 

names(db_long) #ver nombres de variables
glimpse(db_long) # ver los primeros 15 casos de cada variable
dim(db_long) # ver cantidad de filas y columnas 
```

```{r}
#| label: cohort data
#| echo: false

db1 <- db_long %>%
filter(ola == 1) %>% # quedarse con solo la ola 1
select(-c(ola, curse_level, p20, p21_ano, age)) #%>% # eliminar variables que no son parte del analisis
#na.omit() # listwise deletion

```

```{r}
#| label: Initial model

# model_restricted <- '
#perc_merit = ~ perc_effort + perc_talent
#perc_nmerit = ~ perc_rich_parents + perc_contact
#pref_merit = ~ pref_effort + pref_talent
#pref_nmerit = ~ pref_rich_parents + pref_contact
#'
```

```{r}
#| label: Measurement model
#| include: false 

model_restricted <- '
perc_merit = ~ perc_effort + perc_talent
perc_nmerit = ~ a*perc_rich_parents + a*perc_contact
pref_merit = ~ pref_effort + pref_talent
pref_nmerit = ~ pref_rich_parents + pref_contact
perc_effort ~~ pref_talent
'

# En este modelo se establece una restricción de igualdad a los indicadores de percepción no meritocrática ya que, en el primer modelo (chunk anterior), uno de ellos daba una carga estandarizada negativa. Además, los indicadores percepción esfuerzo y preferencia talento covariaban altamente en el modelo anterior, por lo que se decidió especificar su correlación
```

```{r}
#| label: Models by cohort
#| output: false

mgeneral_cfa <- cfa(model = model_restricted, 
                   data = db1, 
                   estimator = "WLSMV",
                   ordered = T,
                   std.lv = F)

mbasica_cfa <- cfa(model = model_restricted, 
                   data = subset(db1, cohort_level == "Primary"), 
                   estimator = "WLSMV",
                   ordered = T,
                   std.lv = F)

mmedia_cfa <- cfa(model = model_restricted, 
                  data = subset(db1, cohort_level == "Secondary"), 
                  estimator = "WLSMV",
                  ordered = T,
                  std.lv = F)

# se estiman 3 modelos CFA: uno con toda la muestra, un segundo modelo de la cohorte de básica y un tercer modelo de la cohorte de media
```

```{r}
#| include: false 

summary(mmedia_cfa, standardized = TRUE, fit.measures = TRUE)
summary(mbasica_cfa, standardized = TRUE, fit.measures = TRUE)
```

```{r}
#| label: tbl-factorload
#| include: false
#| message: false

cnames <- c("Factor","Indicator","Loadings Básica","Loadings Media")
kable(left_join(x = standardizedsolution(mbasica_cfa) %>%
                  filter(op=="=~") %>%
                  select(lhs,rhs,est.std),y = standardizedsolution(mmedia_cfa) %>%
                  filter(op=="=~") %>%
                  select(lhs,rhs,est.std),c("lhs","rhs")),
      format = "markdown",digits = 2,col.names = cnames, caption = "Factor loadings")

# tabla comparativa de las cargas factoriales estandarizadas de las dos cohortes para poder visualizar de manera clara las similitudes y diferencias entre básica y media
```

![](imagenes/esquema-basica.png)

The diagram shows the standardized factor loadings estimated with WLSMV for the primary and secondary cohorts. One of the first findings is that within each cohort, the loadings vary depending on the factor. In the case of the primary education model, the loadings of both meritocratic perception indicators differ as perception of effort at .60 and perception of talent at .71, so the meritocratic perception factor explains talent more than effort. In contrast, in the meritocratic preferences factor, the loadings vary greatly, with preference for effort being .46 and preference for talent being .86. This suggests that the factor explains the second item more than the first. Finally, the indicators of the non-meritocratic preferences factor vary, but not as strongly.

![](imagenes/esquema-media.png)

In the secondary education model, some similarities can be seen in comparison to the primary model. The indicators of the meritocratic perception factor have slightly higher loadings than in the previous model, but they do not differ as much from each other. The difference in the indicators of meritocratic preference is reversed, with preference for effort becoming the highest (.76) and preference for talent the lowest (0.41). With regard to non-meritocratic preferences, the indicators have much more similar factor loadings than in the primary school model.

```{r}
#| label: tbl-fitcohort
#| tbl-cap: Summary fit indices of three models
#| echo: false

fit_measures <- rbind(
  "General" = fitMeasures(mgeneral_cfa,
                           c("chisq", "df", "pvalue", "cfi", "tli", "rmsea", "srmr")),
  "Primary" = fitMeasures(mbasica_cfa,
                         c("chisq", "df", "pvalue", "cfi", "tli", "rmsea", "srmr")),
  "Secondary" = fitMeasures(mmedia_cfa,
                        c("chisq", "df", "pvalue", "cfi", "tli", "rmsea", "srmr"))
)

knitr::kable(fit_measures, digits = 3, caption = "Fit indexes by model")

# tabla de los indices de ajuste de los tres modelos para comparar sus índices de ajuste en bruto
```

@tbl-fitcohort shows the fit indices for each of the three models. All models achieved a non-significant chi-square, which could be expected given their sensitivity to large samples, such as those used in this study.

The first model is the general model, i.e., the one that includes primary and secondary school students. It can be seen that it has good fit indices (CFI=0.989, RMSEA=0.048, $\chi^2$(df=14)=41.026), so we can conclude that the four latent factor scale works well for students.

The second model contains data from primary students. In this case, the fit indices work acceptable (CFI=0.975, RMSEA=0.063, $\chi^2$(df=14)=36. 295). In this case, the scale has less validation than for the previous model.

It is noteworthy that, for the secondary education model, most indicators have values that are close to perfect (CFI=1, RMSEA=0, $\chi^2$(df=14)=11.779). However, the results of this model could be overfitting, so they should be interpreted with caution.

```{r}
#| label: configural model
#| include: false 

baseline <- measEq.syntax(
  configural.model = model_restricted, # se especifica el modelo que se usará
  data = db1,
  ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact", # se explicita que las 
              "pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"),# variables son ordinales 
  parameterization = "delta", #parametrización ideal para indicadores ordinales
  ID.fac = "std.lv", # identidica el factor con varianza 1
  ID.cat = "Wu.Estabrook.2016", # tipo de estrategia ideal para indicadores ordinales
  group = "cohort_level", # identificación de los grupos
  group.equal = "configural" # nivel de invarianza 
)

model.baseline <- as.character(baseline) # se crea un objeto con el modelo configural como caracter para que puedan visualizarse sus datos

fit.baseline <- cfa(model.baseline, data = db1, group = "cohort_level",
ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact", 
              "pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"))

summary(fit.baseline)
```

```{r}
#| label: weak model
#| include: false

# los siguientes dos modelos siguen los pasos de análisis de Dubravka et al, 2019.

thresholds <- measEq.syntax(
configural.model = model_restricted,
data = db1,
ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact",
"pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"),
parameterization = "delta",
ID.fac = "std.lv",
ID.cat = "Wu.Estabrook.2016",
group = "cohort_level",
group.equal = c("thresholds") # aquí se aplica lo mismo que el código anterior excepto que ahora se restringen los thresholds
)

model.thres <- as.character(thresholds)

fit.thres <- cfa(model.thres, data = db1, group = "cohort_level",
ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact",
"pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"))

summary(fit.thres, fit.measures = TRUE)
```

```{r}
#| label: loading-threshold model
#| include: false

strong <- measEq.syntax(
configural.model = model_restricted,
data = db1,
ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact",
"pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"),
parameterization = "delta",
ID.fac = "std.lv",
ID.cat = "Wu.Estabrook.2016",
group = "cohort_level",
group.equal = c("thresholds", "loadings") # ahora se restringen los tresholds y las cargas factoriales
)

model.strong <- as.character(strong)

fit.strong <- cfa(model.strong, data = db1, group = "cohort_level",
ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact",
"pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"))

summary(fit.strong, fit.measures = TRUE)

```

```{r}
#| label: cohort-table
#| echo: false

# Comparaciones de Chi²
an1 <- anova(fit.baseline, fit.thres)
an2 <- anova(fit.thres, fit.strong)

tab01 <- bind_rows(
  as_tibble(an1)[2,],
  as_tibble(an2)[2,] # aqui se toma la segunda fila de comparación de cada anova
) %>%
  select("Chisq", "Df", chisq.diff = `Chisq diff`, df.diff = `Df diff`, pvalue = `Pr(>Chisq)`) %>%
  mutate(
    stars = stars.pval(pvalue),
    chisqt = paste0(round(Chisq, 2), " (", Df, ")"),
    decision = ifelse(pvalue > 0.05, "Accept", "Reject"),
    model = c("Weak", "Strong") # etiqueta los modelos que se extraen de los anova
  ) %>%
  bind_rows(
    tibble(
      Chisq = an1$Chisq[1],
      Df = an1$Df[1],
      chisq.diff = NA,
      df.diff = NA,
      pvalue = NA,
      stars = "",
      chisqt = paste0(round(an1$Chisq[1], 2), " (", an1$Df[1], ")"),
      decision = "Reference",
      model = "Configural" # se añade el modelo configural como referencia para clarificar la comparación
    )
  ) %>%
  select(model, chisqt, chisq.diff, df.diff, pvalue, stars, decision) %>%
  mutate(model = factor(model, levels = c("Configural", "Weak", "Strong"))) %>%
  arrange(model)

# se extraen los índices de ajuste para cada modelo
fit.meas <- bind_rows(
  fitMeasures(fit.baseline, output = "matrix")[c("chisq", "df", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"),],
  fitMeasures(fit.thres, output = "matrix")[c("chisq", "df", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"),],
  fitMeasures(fit.strong, output = "matrix")[c("chisq", "df", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"),]
)

fit.meas <- fit.meas %>% # se calculan las diferencias de cada modelo con el anterior para 
  mutate(                # analizar si se cumplen criterios de invarianza 
    diff.chi2 = chisq - lag(chisq, default = first(chisq)),
    diff.df = df - lag(df, default = first(df)),
    diff.cfi = cfi - lag(cfi, default = first(cfi)),
    diff.rmsea = rmsea - lag(rmsea, default = first(rmsea))
  ) %>%
  round(3) %>%
  mutate(rmsea.ci = paste0(rmsea, " \n(", rmsea.ci.lower, "-", rmsea.ci.upper, ")"))

# Tabla final (mantener diff.df hasta formatear)
tab.inv <- bind_cols(tab01, fit.meas) %>%
  select(model, chisqt, cfi, rmsea.ci, diff.chi2, diff.df, diff.cfi, diff.rmsea, stars, decision) %>%
  mutate(diff.chi2 = paste0(diff.chi2, " (", diff.df, ") ", stars)) %>%
  select(model, chisqt, cfi, rmsea.ci, diff.chi2, diff.cfi, diff.rmsea, decision)

# Encabezados de columna
col.nam <- c(
  "Model", "&chi;^2 (df)", "CFI", "RMSEA (90 CI)",
  "&Delta; &chi;^2 (&Delta; df)", "&Delta; CFI", "&Delta; RMSEA", "Decision"
)

# Se formatea la tabla en kable
tab.inv %>%
  kableExtra::kable(
    format = "html",
    align = "c",
    booktabs = TRUE,
    escape = FALSE,
    col.names = col.nam
  ) %>%
  kableExtra::kable_styling(
    full_width = TRUE,
    latex_options = "hold_position",
    bootstrap_options = c("striped", "bordered", "condensed"),
    font_size = 23
  ) %>%
  kableExtra::column_spec(c(1, 8), width = "3.5cm") %>%
  kableExtra::column_spec(2:7, width = "4cm") %>%
  kableExtra::column_spec(4, width = "5cm")



```

The results of the different invariance models are displayed at the previous table. To examine invariance across cohorts the same steps in Dubravka et al. (2019) were followed, who propose the estimation of three models: the configural model, another restricting the thresholds, and finally restricting the thresholds and factor loadings.

The configural model was first estimated, which maintains the same factor structure for both baseline and midline. The configural model has good fit indices (CFI = 0.992, RMSEA = 0.037), so there is empirical evidence that the factor structure behaves stably in both groups.

Looking at the thresholds restricted model, it appears that when thresholds are restricted to equality, the four-factor latent model is not equivalent across the two cohorts in the study in accordance with @chen_sensitivity_2007 ($\Delta$CFI -.014 \< -. 01); $\Delta$RMSEA .019 \> .015). This result implies that by restricting thresholds, the meritocracy scale varies between primary and secondary education. In this case, invariance is not satisfied.

The level represented as strong restricts both thresholds and factor loadings. When compared with the previous level of invariance, it can be seen that the criteria for assuming that the meritocracy scale remains stable across cohorts are not met either. ($\Delta$CFI -.005 \< -. 01); $\Delta$RMSEA .003 \< .015), so invariance is rejected.

It is pertinent to ask to what extent these results are due to the instability of the secondary education model. An attempt was made to resolve its overfitting, but this was not possible, which may have had direct implications for this part of the analysis.

