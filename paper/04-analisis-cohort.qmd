---
author: "Equipo EDUMER"
bibliography: "../input/bib/merit-factorial.bib"
csl: "../input/bib/apa6.csl"
---

## Cohort Invariance Test

```{r}
#| label: set
#| echo: false
#| message: false
#| warning: false

library(knitr)

knitr::opts_chunk$set(echo = F,
                      warning = F,
                      error = F, 
                      message = F) 

table_format <- if (is_html_output()) {
  "html"
} else if (is_latex_output()) {
  "latex"
}
table_format2 <- if (is_html_output()) {
  T
} else if (is_latex_output()) {
  F
}

options(kableExtra.html.bsTable = T)
options(knitr.kable.NA = "")

# funcion para exportar tablas en formato html o latex
```

```{r}
#| label: packages
#| include: false
#| echo: false

if (!require("pacman")) install.packages("pacman") #para instalar pacman en caso de no estar instalado

pacman::p_load(
  tidyverse,
  sjmisc,
  sjPlot,
  here,
  lavaan,
  psych,
  ggdist,
  semTools,
  gtools,
  kableExtra
)

options(scipen = 999) # para eliminar la notación científica
rm(list = ls()) # limpiar el ambiente 
```

```{r}
#| label: longitudinal data 
#| echo: false
#| output: false

load(file = here("output", "data", "db_long_proc.RData")) #cargar base con las dos olas 

names(db_long) #ver nombres de variables
glimpse(db_long) # ver los primeros 15 casos de cada variable
dim(db_long) # ver cantidad de filas y columnas 
```

```{r}
#| label: cohort data
#| echo: false

db1 <- db_long %>%
filter(ola == 1) %>% # quedarse con solo la ola 1
select(-c(ola, curse_level, p20, p21_ano, age)) #%>% # eliminar variables que no son parte del analisis
#na.omit() # listwise deletion

```

```{r}
#| label: Initial model

# model_restricted <- '
#perc_merit = ~ perc_effort + perc_talent
#perc_nmerit = ~ perc_rich_parents + perc_contact
#pref_merit = ~ pref_effort + pref_talent
#pref_nmerit = ~ pref_rich_parents + pref_contact
#'
```

```{r}
#| label: Measurement model
#| include: false 

model_restricted <- '
perc_merit = ~ perc_effort + perc_talent
perc_nmerit = ~ a*perc_rich_parents + a*perc_contact
pref_merit = ~ pref_effort + pref_talent
pref_nmerit = ~ pref_rich_parents + pref_contact
perc_effort ~~ pref_talent
'

# En este modelo se establece una restricción de igualdad a los indicadores de percepción no meritocrática ya que, en el primer modelo (chunk anterior), uno de ellos daba una carga estandarizada negativa. Además, los indicadores percepción esfuerzo y preferencia talento covariaban altamente en el modelo anterior, por lo que se decidió especificar su correlación
```

```{r}
#| label: Models by cohort
#| output: false

mgeneral_cfa <- cfa(model = model_restricted, 
                   data = db1, 
                   estimator = "WLSMV",
                   ordered = T,
                   std.lv = F)

mbasica_cfa <- cfa(model = model_restricted, 
                   data = subset(db1, cohort_level == "Primary"), 
                   estimator = "WLSMV",
                   ordered = T,
                   std.lv = F)

mmedia_cfa <- cfa(model = model_restricted, 
                  data = subset(db1, cohort_level == "Secondary"), 
                  estimator = "WLSMV",
                  ordered = T,
                  std.lv = F)

# se estiman 3 modelos CFA: uno con toda la muestra, un segundo modelo de la cohorte de básica y un tercer modelo de la cohorte de media
```

```{r}
#| include: false 

summary(mmedia_cfa, standardized = TRUE, fit.measures = TRUE)
summary(mbasica_cfa, standardized = TRUE, fit.measures = TRUE)
```

```{r}
#| label: tbl-factorload
#| include: false
#| message: false

cnames <- c("Factor","Indicator","Loadings Básica","Loadings Media")
kable(left_join(x = standardizedsolution(mbasica_cfa) %>%
                  filter(op=="=~") %>%
                  select(lhs,rhs,est.std),y = standardizedsolution(mmedia_cfa) %>%
                  filter(op=="=~") %>%
                  select(lhs,rhs,est.std),c("lhs","rhs")),
      format = "markdown",digits = 2,col.names = cnames, caption = "Factor loadings")

# tabla comparativa de las cargas factoriales estandarizadas de las dos cohortes para poder visualizar de manera clara las similitudes y diferencias entre básica y media
```

```{r}
#| label: fig-cohortes
#| fig-cap: Multigroup model diagram
#| fig-cap-location: bottom
#| echo: false
#| out.width: 100%
#| out.height: "auto" 

knitr::include_graphics('imagenes/esquema-cohortes.png')
```


@fig-cohortes presents the standardized factor loadings estimated using the WLSMV estimator for the primary and secondary cohorts. The text highlighted in red corresponds to the primary education model, while the text in black refers to the secondary education model. A general result is that, within each cohort, the magnitude of the loadings differs across latent factors. 

In the primary education model, meritocratic perception indicators show unequal loadings: the perception of effort has a loading of 0.60, while the perception of talent reaches 0.71. This pattern indicates that the latent factor is more strongly associated with perceptions of talent than with those of effort. A more pronounced discrepancy is observed in the meritocratic preferences factor, where the preference for effort shows a relatively low loading (0.46), in contrast to a substantially higher factor loading for the preference for talent (0.86), suggesting that the factor is mainly driven by the latter indicator. In comparison, the indicators of non-meritocratic preferences also vary greatly, with the indicator of preference for contacts exceeding preferences for wealthy families by 0.2. Finally, the non-meritocratic perceptions factor does not show variability since its indicators were subject to an equality constraint in the model specification. This decision was made because without this constraint, the perception of contacts yielded a negative loading. 

The secondary education model shows both continuities and contrasts with the primary model. Meritocratic perception indicators follow the same pattern as the previous model, where talent carries more weight than effort. In the case of meritocratic preferences, the pattern observed in the primary education cohort is reversed: the preference for effort has the highest weight (0.76), while the preference for talent has the lowest (0.41). With regard to the factor of non-meritocratic perceptions, both indicators have the same weight due to the aforementioned equality restriction. Finally, the loadings of the non-meritocratic preference indicators are slightly higher overall and more homogeneous, indicating a more balanced contribution of preferences for wealthy families and contacts to the latent construct.

```{r}
#| label: tbl-fitcohort
#| tbl-cap: Summary fit indices of three models
#| echo: false

fit_measures <- rbind(
  "General" = fitMeasures(mgeneral_cfa,
                           c("chisq", "df", "pvalue", "cfi", "tli", "rmsea", "srmr")),
  "Primary" = fitMeasures(mbasica_cfa,
                         c("chisq", "df", "pvalue", "cfi", "tli", "rmsea", "srmr")),
  "Secondary" = fitMeasures(mmedia_cfa,
                        c("chisq", "df", "pvalue", "cfi", "tli", "rmsea", "srmr"))
)

knitr::kable(fit_measures, digits = 3)

# tabla de los indices de ajuste de los tres modelos para comparar sus índices de ajuste en bruto
```

@tbl-fitcohort presents the goodness-of-fit indices for each of the three estimated models. In all three cases, the chi-square statistic is not significant, which is to be expected given its known sensitivity to large sample sizes, such as those used in this study; therefore, the overall goodness-of-fit assessment is based mainly on incremental and parsimony indices.

The first model corresponds to the general model, which simultaneously incorporates primary and secondary school students. This model has adequate fit indices (CFI = 0.989; RMSEA = 0.048; $\chi²$(14) = 41.026), suggesting that the four-factor latent structure provides an empirically satisfactory representation of the construct in the total sample. Consequently, there is preliminary evidence in favor of the structural validity of the factorial model in the student group. Second model was estimated exclusively with data from primary school students. In this case, the fit indices remain within acceptable ranges, although with a lower performance than the general model (CFI = 0.975; RMSEA = 0.063; $\chi²$(14) = 36.295), suggesting a moderate fit and a lower adequacy of the factorial model in this specific subgroup. Finally, the model estimated for secondary school students shows virtually perfect fit indices (CFI = 1.00; RMSEA = 0.00; $\chi²$(14) = 11.779). However, this extreme fit pattern is indicative of a possible overfitting of the model to the data, so the results should be interpreted with caution from a psychometric perspective.

Taken together, these findings suggest that, although the proposed factor structure is adequate at the global level, its performance varies across educational cohorts, reinforcing the need to formally assess measurement invariance before making substantive comparisons between groups.

```{r}
#| label: configural model
#| include: false 

baseline <- measEq.syntax(
  configural.model = model_restricted, # se especifica el modelo que se usará
  data = db1,
  ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact", # se explicita que las 
              "pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"),# variables son ordinales 
  parameterization = "delta", #parametrización ideal para indicadores ordinales
  ID.fac = "std.lv", # identidica el factor con varianza 1
  ID.cat = "Wu.Estabrook.2016", # tipo de estrategia ideal para indicadores ordinales
  group = "cohort_level", # identificación de los grupos
  group.equal = "configural" # nivel de invarianza 
)

model.baseline <- as.character(baseline) # se crea un objeto con el modelo configural como caracter para que puedan visualizarse sus datos

fit.baseline <- cfa(model.baseline, data = db1, group = "cohort_level",
ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact", 
              "pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"))

summary(fit.baseline)
```

```{r}
#| label: weak model
#| include: false

# los siguientes dos modelos siguen los pasos de análisis de Dubravka et al, 2019.

thresholds <- measEq.syntax(
configural.model = model_restricted,
data = db1,
ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact",
"pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"),
parameterization = "delta",
ID.fac = "std.lv",
ID.cat = "Wu.Estabrook.2016",
group = "cohort_level",
group.equal = c("thresholds") # aquí se aplica lo mismo que el código anterior excepto que ahora se restringen los thresholds
)

model.thres <- as.character(thresholds)

fit.thres <- cfa(model.thres, data = db1, group = "cohort_level",
ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact",
"pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"))

summary(fit.thres, fit.measures = TRUE)
```

```{r}
#| label: loading-threshold model
#| include: false

strong <- measEq.syntax(
configural.model = model_restricted,
data = db1,
ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact",
"pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"),
parameterization = "delta",
ID.fac = "std.lv",
ID.cat = "Wu.Estabrook.2016",
group = "cohort_level",
group.equal = c("thresholds", "loadings") # ahora se restringen los tresholds y las cargas factoriales
)

model.strong <- as.character(strong)

fit.strong <- cfa(model.strong, data = db1, group = "cohort_level",
ordered = c("perc_effort", "perc_talent", "perc_rich_parents", "perc_contact",
"pref_effort", "pref_talent", "pref_rich_parents", "pref_contact"))

summary(fit.strong, fit.measures = TRUE)

```

```{r}
#| echo: false
#| label: tbl-cohort-table
#| tbl-cap: Comparative table of multi-group invariance
#| tbl-cap-location: top

# Comparaciones de Chi²
an1 <- anova(fit.baseline, fit.thres)
an2 <- anova(fit.thres, fit.strong)

tab01 <- bind_rows(
  as_tibble(an1)[2,],
  as_tibble(an2)[2,] # aqui se toma la segunda fila de comparación de cada anova
) %>%
  select("Chisq", "Df", chisq.diff = `Chisq diff`, df.diff = `Df diff`, pvalue = `Pr(>Chisq)`) %>%
  mutate(
    stars = stars.pval(pvalue),
    chisqt = paste0(round(Chisq, 2), " (", Df, ")"),
    decision = ifelse(pvalue > 0.05, "Accept", "Reject"),
    model = c("Tresholds", "Tresholds + Loadings") # etiqueta los modelos que se extraen de los anova
  ) %>%
  bind_rows(
    tibble(
      Chisq = an1$Chisq[1],
      Df = an1$Df[1],
      chisq.diff = NA,
      df.diff = NA,
      pvalue = NA,
      stars = "",
      chisqt = paste0(round(an1$Chisq[1], 2), " (", an1$Df[1], ")"),
      decision = "Reference",
      model = "Configural" # se añade el modelo configural como referencia para clarificar la comparación
    )
  ) %>%
  select(model, chisqt, chisq.diff, df.diff, pvalue, stars, decision) %>%
  mutate(model = factor(model, levels = c("Configural", "Tresholds", "Tresholds + Loadings"))) %>%
  arrange(model)

# se extraen los índices de ajuste para cada modelo
fit.meas <- bind_rows(
  fitMeasures(fit.baseline, output = "matrix")[c("chisq", "df", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"),],
  fitMeasures(fit.thres, output = "matrix")[c("chisq", "df", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"),],
  fitMeasures(fit.strong, output = "matrix")[c("chisq", "df", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"),]
)

fit.meas <- fit.meas %>% # se calculan las diferencias de cada modelo con el anterior para 
  mutate(                # analizar si se cumplen criterios de invarianza 
    diff.chi2 = chisq - lag(chisq, default = first(chisq)),
    diff.df = df - lag(df, default = first(df)),
    diff.cfi = cfi - lag(cfi, default = first(cfi)),
    diff.rmsea = rmsea - lag(rmsea, default = first(rmsea))
  ) %>%
  round(3) %>%
  mutate(rmsea.ci = paste0(rmsea, " \n(", rmsea.ci.lower, "-", rmsea.ci.upper, ")"))

# Tabla final (mantener diff.df hasta formatear)
tab.inv <- bind_cols(tab01, fit.meas) %>%
  select(model, chisqt, cfi, rmsea.ci, diff.chi2, diff.df, diff.cfi, diff.rmsea, stars, decision) %>%
  mutate(diff.chi2 = paste0(diff.chi2, " (", diff.df, ") ", stars)) %>%
  select(model, chisqt, cfi, rmsea.ci, diff.chi2, diff.cfi, diff.rmsea, decision)

# Encabezados de columna
col.nam <- c(
  "Model", "&chi;^2 (df)", "CFI", "RMSEA (90 CI)",
  "&Delta; &chi;^2 (&Delta; df)", "&Delta; CFI", "&Delta; RMSEA", "Decision"
)

# Se formatea la tabla en kable
tab.inv %>%
  kableExtra::kable(
    format = "html",
    align = "c",
    booktabs = TRUE,
    escape = FALSE,
    col.names = col.nam
  ) %>%
  kableExtra::kable_styling(
    full_width = TRUE,
    latex_options = "hold_position",
    bootstrap_options = c("striped", "bordered", "condensed"),
    font_size = 23
  ) %>%
  kableExtra::column_spec(c(1, 8), width = "3.5cm") %>%
  kableExtra::column_spec(2:7, width = "4cm") %>%
  kableExtra::column_spec(4, width = "5cm")

```


The results of the different invariance models are presented in the @tbl-cohort-table. To assess invariance between cohorts, the sequential procedure proposed by @svetina_multiplegroup_2020 was followed, which involves estimating three hierarchical models: (1) a configural model, (2) a model with threshold restrictions, and (3) a final model that simultaneously imposes equality of thresholds and factor loadings between groups.

First, the configural model was estimated, which assumes an identical factor structure in both cohorts (primary and secondary education), without imposing equality restrictions on the parameters. This model presented excessively high fit indices (CFI = 1.00; RMSEA = 0.00), suggesting overfitting and, therefore, insufficient empirical evidence to support the stability of the factor structure between the compared groups.

In a second stage, the model was estimated with restricted thresholds. According to the sample size sensitivity criteria proposed by @chen_sensitivity_2007, the four-latent-factor model does not achieve equivalence between cohorts ($\Delta$CFI = −.014 < −.01; $\Delta$RMSEA = .019 > .015). This result indicates that, when imposing equality of thresholds, the meritocracy scale does not show invariance between primary and secondary school students, suggesting systematic differences in the location of response categories between the two groups.

Finally, the third model was evaluated with simultaneous restrictions on thresholds and factor loadings. The changes observed in the fit indices ($\Delta$CFI = −.003; $\Delta$RMSEA = .006) also do not meet the criteria established for assuming metric and scalar invariance. Consequently, this model must also be considered non-invariant. Overall, the results indicate that the meritocracy scale does not present measurement invariance between the cohorts analyzed, which limits the direct comparability of latent scores between educational levels.

