---
author: "Equipo EDUMER"
bibliography: "../input/bib/merit-factorial.bib"
csl: "../input/bib/apa6.csl"
---

# Results

```{r}
#| label: set
#| echo: false
#| message: false
#| warning: false

library(knitr)
knitr::opts_chunk$set(echo = TRUE, include = TRUE, warning = FALSE, message = FALSE)

table_format <- if (is_html_output()) {
  "html"
} else if (is_latex_output()) {
  "latex"
}
table_format2 <- if (is_html_output()) {
  T
} else if (is_latex_output()) {
  F
}

options(kableExtra.html.bsTable = T)
options(knitr.kable.NA = "")
```

```{r}
#| label: packages
#| include: false
#| echo: false

if (!require("pacman")) install.packages("pacman")

pacman::p_load(
  tidyverse,
  sjmisc,
  sjPlot,
  here,
  lavaan,
  psych,
  corrplot,
  ggdist,
  patchwork,
  semTools,
  gtools,
  kableExtra
)

options(scipen = 999)
rm(list = ls())
```

```{r}
#| label: longitudinal data 
#| echo: false
#| output: false

load(file = here("output", "data", "db_long_proc.RData"))

names(db_long)
glimpse(db_long)
dim(db_long)
```

```{r}
#| label: cohort data

load(file = here("output/data/db1_proc.RData"))
```


## Cohort Invariance Test

```{r}
#| label: Measurement model

model_cfa <- '
  perc_merit = ~ perc_effort + perc_talent
  perc_nmerit = ~ perc_rich_parents + perc_contact
  pref_merit = ~ pref_effort + pref_talent
  pref_nmerit = ~ pref_rich_parents + pref_contact
  '

```

```{r}
#| include: false 

m1_cfa <- cfa(model = model_cfa, 
              data = db1,
              estimator = "MLR", 
              std.lv = F) # Continuous/ estimator ML Robust

m2_cfa <- cfa(model = model_cfa, 
              data = db1, 
              estimator = "DWLS",
              ordered = T,
              std.lv = F)
```

```{r}
#| label: Factor loadings
#| include: false 
#| message: false

cnames <- c("Factor","Indicator","Loading (MLR)","Loading (DWLS)")
kable(left_join(x = standardizedsolution(m1_cfa) %>% 
                  filter(op=="=~") %>% 
                  select(lhs,rhs,est.std),y = standardizedsolution(m2_cfa) %>% 
                  filter(op=="=~") %>%
                  select(lhs,rhs,est.std),c("lhs","rhs")),
      format = "markdown",digits = 2,col.names = cnames, caption = "Factor loadings")
```

```{r}
#| label: Models by cohort

mgeneral_cfa <- cfa(model = model_cfa, 
                   data = db1, 
                   estimator = "DWLS",
                   ordered = T,
                   std.lv = F)

mbasica_cfa <- cfa(model = model_cfa, 
                   data = subset(db1, curse_level == "Básica"), 
                   estimator = "DWLS",
                   ordered = T,
                   std.lv = F)

mmedia_cfa <- cfa(model = model_cfa, 
                  data = subset(db1, curse_level == "Media"), 
                  estimator = "DWLS",
                  ordered = T,
                  std.lv = F)
```

```{r}
#| label: tbl-factorload
#| echo: false
#| message: false

cnames <- c("Factor","Indicator","Loadings Básica","Loadings Media")
kable(left_join(x = standardizedsolution(mbasica_cfa) %>%
                  filter(op=="=~") %>%
                  select(lhs,rhs,est.std),y = standardizedsolution(mmedia_cfa) %>%
                  filter(op=="=~") %>%
                  select(lhs,rhs,est.std),c("lhs","rhs")),
      format = "markdown",digits = 2,col.names = cnames, caption = "Factor loadings")

```

Based on the four latent factor model, the analysis consisted of estimating and comparing the fit indicators of three models: a general model, one for elementary school students, and another for high school students.

@tbl-factorload shows the standardized factor loadings, estimated with DWLS, for the models for primary and secondary education. The loadings vary greatly depending on the indicator. In the meritocratic preferences factor, the factor loading for preference for effort in the primary model is 0.45, while in the secondary model it is 0.83. This means that the factor in the secondary model is much stronger, explaining almost twice as much of the indicator as in the primary model. Within the same factor is the indicator of preferences for talent, which in primary education has a factor loading of 0.88, suffering a considerable decline in the secondary education model, scoring 0.37. In this sense, the item of preference for talent poorly measures the factor of meritocratic preferences in the secondary education model, but not in the primary education model. 

A general case in both models is the high factor loading of the perception of contacts item in the non-meritocratic perceptions factor, with 0.94 in the basic model and 1.01 in the average model. It should be noted that this table reflects standardized loadings, so this loading in the average model is an anomalous result, which could cause problems in subsequent estimates.

```{r}
#| label: tbl-fitcohort
#| tbl-cap: Summary fit indices of three models
#| echo: false

fit_measures <- rbind(
  "General" = fitMeasures(mgeneral_cfa,
                           c("chisq", "df", "pvalue", "cfi", "tli", "rmsea", "srmr")),
  "Primary" = fitMeasures(mbasica_cfa,
                         c("chisq", "df", "pvalue", "cfi", "tli", "rmsea", "srmr")),
  "Secondary" = fitMeasures(mmedia_cfa,
                        c("chisq", "df", "pvalue", "cfi", "tli", "rmsea", "srmr"))
)

knitr::kable(fit_measures, digits = 3, caption = "Fit indexes by model")

```

@tbl-fitcohort shows the fit indices for each of the three models. All models achieved a non-significant chi-square, which could be expected given their sensitivity to large samples, such as those used in this study. 

The first model is the general model, i.e., the one that includes primary and secondary school students.  It can be seen that it has good fit indices (CFI=0.989, RMSEA=0.046, $\chi^2$(df=14)=39.183), so we can conclude that the four latent factor scale works well for students.

The second model contains data from primary students. In this case, the adjustment indicators work excellently (CFI=0.996, RMSEA=0.025, $\chi^2$(df=14)=17. 430), making it the model with the best fit among those that can be seen.

It is noteworthy that, for the secondary education model, most indicators have values that are close to perfect (CFI=1.0, TLI=1.003, RMSEA=0, $\chi^2$(df=14)=11.779). However, the results of this model could be overfitting, so they should be interpreted with caution.  


```{r}
#| label: Invariance models
#| echo: false

fit.conf <- cfa(model_cfa, data = db1, group = "curse_level")

fit.metric <- cfa(model_cfa, data = db1, group = "curse_level",
                  group.equal = c("loadings"))

fit.strong <- cfa(model_cfa, data = db1, group = "curse_level",
                  group.equal = c("loadings", "intercepts"))

fit.strict <- cfa(model_cfa, data = db1, group = "curse_level",
                  group.equal = c("loadings", "intercepts", "residuals"))

```

```{r}
#| label: tbl-cohortinv
#| echo: false

# Crear la tabla de comparación de modelos
tab01 <- lavaan::anova(fit.conf, fit.metric, fit.strong, fit.strict, SB.classic = TRUE) %>%
  dplyr::as_tibble() %>%
  dplyr::select("Chisq", "Df", "chisq_diff" = `Chisq diff`, "df_diff" = `Df diff`, "pvalue" = `Pr(>Chisq)`) %>%
  dplyr::mutate(stars = gtools::stars.pval(pvalue),
                chisqt = paste0(round(Chisq, 2), " (", Df, ") "),
                decision = ifelse(pvalue > 0.05, yes = "Accept", no = "Reject"),
                model = c("Configural", "Metric", "Strong", "Strict"))

# Extraer medidas de ajuste para cada modelo
fit.meas <- dplyr::bind_rows(
  lavaan::fitmeasures(fit.conf, output = "matrix")[c("chisq", "df", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"), ],
  lavaan::fitmeasures(fit.metric, output = "matrix")[c("chisq", "df", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"), ],
  lavaan::fitmeasures(fit.strong, output = "matrix")[c("chisq", "df", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"), ],
  lavaan::fitmeasures(fit.strict, output = "matrix")[c("chisq", "df", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper"), ]
)

# Calcular diferencias en chisq, df, cfi y rmsea
fit.meas <- fit.meas %>%
  dplyr::mutate(
    diff.chi2 = chisq - lag(chisq, default = dplyr::first(chisq)),
    diff.df = df - lag(df, default = dplyr::first(df)),
    diff.cfi = cfi - lag(cfi, default = dplyr::first(cfi)),
    diff.rmsea = rmsea - lag(rmsea, default = dplyr::first(rmsea))
  ) %>%
  round(3) %>%
  dplyr::mutate(rmsea.ci = paste0(rmsea, " \\n ", "(", rmsea.ci.lower, "-", rmsea.ci.upper, ")"))

# Combinar tablas
tab.inv <- dplyr::bind_cols(tab01, fit.meas) %>%
  dplyr::select(model, chisqt, cfi, rmsea.ci, diff.chi2, diff.df, diff.cfi, diff.rmsea, stars, decision) %>%
  dplyr::mutate(diff.chi2 = paste0(diff.chi2, " (", diff.df, ") ", stars)) %>%
  dplyr::select(model, chisqt, cfi, rmsea.ci, diff.chi2, diff.cfi, diff.rmsea, decision)

# Limpiar valores
tab.inv[tab.inv == "0 (0) "] <- NA
tab.inv[tab.inv == 0] <- NA

# Nombres de columnas
col.nam <- c("Model", "χ² (df)", "CFI", "RMSEA (90% CI)",
             "Δ χ² (Δ df)", "Δ CFI", "Δ RMSEA", "Decision")

# Nota al pie con información del tamaño muestral
nobs <- lavaan::lavInspect(fit.conf, what = "nobs")
footnote <- paste0("N = ", sum(nobs), "; Group 1, n = ", nobs[1], "; Group 2, n = ", nobs[2])

# Crear tabla HTML
knitr::kable(tab.inv, col.names = col.nam, align = "l",
             booktabs = TRUE, format = "html", escape = FALSE,
             caption = "Measurement Invariance by Course Level") %>%
  kableExtra::kable_styling(full_width = FALSE,
                            latex_options = "HOLD_position",
                            bootstrap_options = c("striped", "bordered"),
                            font_size = 10) %>%
  kableExtra::footnote(general = footnote, footnote_as_chunk = TRUE)

```

The results of the different invariance models are displayed at @tbl-cohortinv. To examine invariance across cohorts, the configural model was first estimated, which maintains the same factor structure for both baseline and midline. The configural model has good fit indices (CFI = 0.001, RMSEA = 0.027), so there is empirical evidence that the factor structure behaves stably in both groups.

Looking at the metric model, it appears that when factor loadings are restricted to equality, the four-factor latent model is not equivalent across the different cohorts in the study, despite meeting the $\Delta$CFI criteria, which mean rejecting the model if > 0. 01, as well as the $\Delta$RMSEA being below the cutoff point ($\Delta$RMSEA > 0.015 is rejected). The problem is the p-value < 0.05, which indicates that there are significant differences between the two groups. In this sense, as the following models restrict more parameters, their fit becomes more complex, which is reflected in the fact that none of the following models are accepted. This means that the measurement of the meritocracy scale for students varies depending on the cohort.

## Longitudinal Invariance Test

```{r}
#| label: data 2
#| tbl-cap: "Longitudinal Invariance Test Results 1"
#| results: asis
#| echo: false
#| warning: false

db_invariance <-
  db_long %>%
  select(id_estudiante, ola, starts_with(c("perc", "pref"))) %>%
  pivot_wider(
    id_cols = id_estudiante,
    names_from = ola,
    names_glue = "{.value}{ola}",
    values_from = c(
      perc_effort, perc_talent,
      perc_rich_parents, perc_contact,
      pref_effort, pref_talent,
      pref_rich_parents, pref_contact
    )
  ) %>%
  na.omit() %>%
  rename_with(~ str_replace_all(., "_", ""))

names(db_invariance)
names(db_long)

db_invariance <- db_invariance %>% 
  mutate(
    across(
      .cols = -idestudiante,
      .fns = ~ case_when(. == 1 ~ 0,
                         . == 2 ~ 1,
                         . == 3 ~ 2,
                         . == 4 ~ 3)
    )
  )

db_invariance <- db_invariance %>% 
  mutate(
    across(
      .cols = -idestudiante,
      .fns = ~ ordered(.)
    )
  )

```

```{r}
#| label: conf-model
#| tbl-cap: "Longitudinal Invariance Test"
#| results: asis
#| echo: false
#| warning: false

# First, define the configural model, using the repeated measures factors and
# indicators
baseline_model_smt <- ('

###########################################
# Definición de factores (1 marcador por factor)
###########################################

percmerit1  =~ 1*perceffort1     + perctalent1
percmerit2  =~ 1*perceffort2     + perctalent2

percnmerit1 =~ 1*percrichparents1 + perccontact1
percnmerit2 =~ 1*percrichparents2 + perccontact2

prefmerit1  =~ 1*prefeffort1     + preftalent1
prefmerit2  =~ 1*prefeffort2     + preftalent2

prefnmerit1 =~ 1*prefrichparents1 + prefcontact1
prefnmerit2 =~ 1*prefrichparents2 + prefcontact2


###########################################
# Covarianzas entre errores (mismo ítem, distintas olas)
###########################################

perceffort1 ~~ perceffort2
perctalent1 ~~ perctalent2

percrichparents1 ~~ percrichparents2
perccontact1     ~~ perccontact2

prefeffort1 ~~ prefeffort2
preftalent1 ~~ preftalent2

prefrichparents1 ~~ prefrichparents2
prefcontact1     ~~ prefcontact2
')



# Model Estimation
fit_baseline <- cfa(baseline_model_smt, data = db_invariance,
                    ordered = c("perceffort1", "perctalent1", "perceffort2", "perctalent2",
                                "percrichparents1", "perccontact1", "percrichparents2", "perccontact2",
                                "prefeffort1", "preftalent1", "prefeffort2", "preftalent2",
                                "prefrichparents1", "prefcontact1", "prefrichparents2", "prefcontact2"),
                    parameterization = "theta",
                    estimator = "WLSMV")
```

```{r}
#| label: Weak-Model
#| tbl-cap: "Longitudinal Invariance Weak Model"
#| results: asis
#| echo: false
#| warning: false

loadinginv_model_smt <- ('

# Igualación de cargas (invarianza débil)

percmerit1  =~ 1*perceffort1 + pe_loading*perctalent1
percmerit2  =~ 1*perceffort2 + pe_loading*perctalent2

percnmerit1 =~ 1*percrichparents1 + prc_loading*perccontact1
percnmerit2 =~ 1*percrichparents2 + prc_loading*perccontact2

prefmerit1  =~ 1*prefeffort1 + pfe_loading*preftalent1
prefmerit2  =~ 1*prefeffort2 + pfe_loading*preftalent2

prefnmerit1 =~ 1*prefrichparents1 + pfrc_loading*prefcontact1
prefnmerit2 =~ 1*prefrichparents2 + pfrc_loading*prefcontact2


# Varianzas y covarianzas latentes

percmerit1  ~~ percmerit1 + percmerit2
percmerit2  ~~ percmerit2
percnmerit1 ~~ percnmerit1 + percnmerit2
percnmerit2 ~~ percnmerit2
prefmerit1  ~~ prefmerit1 + prefmerit2
prefmerit2  ~~ prefmerit2
prefnmerit1 ~~ prefnmerit1 + prefnmerit2
prefnmerit2 ~~ prefnmerit2


# Umbrales (uno igualado por ítem)

# perceffort1 | pe1t1*t1 + pe1t2*t2
# perceffort2 | pe2t1*t1 + pe2t2*t2
# perctalent1 | pt1t1*t1 + pt1t2*t2
# perctalent2 | pt2t1*t1 + pt2t2*t2
# percrichparents1 | prp1t1*t1 + prp1t2*t2
# percrichparents2 | prp2t1*t1 + prp2t2*t2
# perccontact1 | pc1t1*t1 + pc1t2*t2
# perccontact2 | pc2t1*t1 + pc2t2*t2
# prefeffort1 | pre1t1*t1 + pre1t2*t2
# prefeffort2 | pre2t1*t1 + pre2t2*t2
# preftalent1 | prt1t1*t1 + prt1t2*t2
# preftalent2 | prt2t1*t1 + prt2t2*t2
# prefrichparents1 | pfrp1t1*t1 + pfrp1t2*t2
# prefrichparents2 | pfrp2t1*t1 + pfrp2t2*t2
# prefcontact1 | pfc1t1*t1 + pfc1t2*t2
# prefcontact2 | pfc2t1*t1 + pfc2t2*t2
 

# Covarianzas entre errores

perceffort1 ~~ perceffort2
perctalent1 ~~ perctalent2
percrichparents1 ~~ percrichparents2
perccontact1 ~~ perccontact2
prefeffort1 ~~ prefeffort2
preftalent1 ~~ preftalent2
prefrichparents1 ~~ prefrichparents2
prefcontact1 ~~ prefcontact2
')


fit_loadinginv <- cfa(loadinginv_model_smt, data = db_invariance,
                      ordered = c("perceffort1", "perctalent1", "perceffort2", "perctalent2",
                                  "percrichparents1", "perccontact1", "percrichparents2", "perccontact2",
                                  "prefeffort1", "preftalent1", "prefeffort2", "preftalent2",
                                  "prefrichparents1", "prefcontact1", "prefrichparents2", "prefcontact2"),
                      parameterization = "theta",
                      estimator = "WLSMV")
```

```{r}
#| label: strong-model
#| tbl-cap: "Longitudinal Invariance Strong Model"
#| results: asis
#| echo: false
#| warning: false

thresholdinv_model_smt <- ('

# Igualación de cargas (como en el modelo débil)

percmerit1  =~ 1*perceffort1 + pe_loading*perctalent1
percmerit2  =~ 1*perceffort2 + pe_loading*perctalent2

percnmerit1 =~ 1*percrichparents1 + prc_loading*perccontact1
percnmerit2 =~ 1*percrichparents2 + prc_loading*perccontact2

prefmerit1  =~ 1*prefeffort1 + pfe_loading*preftalent1
prefmerit2  =~ 1*prefeffort2 + pfe_loading*preftalent2

prefnmerit1 =~ 1*prefrichparents1 + pfrc_loading*prefcontact1
prefnmerit2 =~ 1*prefrichparents2 + pfrc_loading*prefcontact2


# Varianzas y covarianzas latentes

percmerit1  ~~ percmerit1 + percmerit2
percmerit2  ~~ percmerit2
percnmerit1 ~~ percnmerit1 + percnmerit2
percnmerit2 ~~ percnmerit2
prefmerit1  ~~ prefmerit1 + prefmerit2
prefmerit2  ~~ prefmerit2
prefnmerit1 ~~ prefnmerit1 + prefnmerit2
prefnmerit2 ~~ prefnmerit2


# Thresholds iguales entre olas (invarianza fuerte)

perceffort1     | pe_t1*t1 + pe_t2*t2
perceffort2     | pe_t1*t1 + pe_t2*t2
perctalent1     | pt_t1*t1 + pt_t2*t2
perctalent2     | pt_t1*t1 + pt_t2*t2
percrichparents1| prp_t1*t1 + prp_t2*t2
percrichparents2| prp_t1*t1 + prp_t2*t2
perccontact1    | pc_t1*t1 + pc_t2*t2
perccontact2    | pc_t1*t1 + pc_t2*t2
prefeffort1     | pre_t1*t1 + pre_t2*t2
prefeffort2     | pre_t1*t1 + pre_t2*t2
preftalent1     | prt_t1*t1 + prt_t2*t2
preftalent2     | prt_t1*t1 + prt_t2*t2
prefrichparents1| pfrp_t1*t1 + pfrp_t2*t2
prefrichparents2| pfrp_t1*t1 + pfrp_t2*t2
prefcontact1    | pfc_t1*t1 + pfc_t2*t2
prefcontact2    | pfc_t1*t1 + pfc_t2*t2


# Interceptos fijos

perceffort1 + perctalent1 ~ 0*1
perceffort2 + perctalent2 ~ 0*1
percrichparents1 + perccontact1 ~ 0*1
percrichparents2 + perccontact2 ~ 0*1
prefeffort1 + preftalent1 ~ 0*1
prefeffort2 + preftalent2 ~ 0*1
prefrichparents1 + prefcontact1 ~ 0*1
prefrichparents2 + prefcontact2 ~ 0*1


# Varianzas únicas

perceffort1 ~~ 1*perceffort1
perctalent1 ~~ 1*perctalent1
percrichparents1 ~~ 1*percrichparents1
perccontact1 ~~ 1*perccontact1
prefeffort1 ~~ 1*prefeffort1
preftalent1 ~~ 1*preftalent1
prefrichparents1 ~~ 1*prefrichparents1
prefcontact1 ~~ 1*prefcontact1

perceffort2 ~~ NA*perceffort2
perctalent2 ~~ NA*perctalent2
percrichparents2 ~~ NA*percrichparents2
perccontact2 ~~ NA*perccontact2
prefeffort2 ~~ NA*prefeffort2
preftalent2 ~~ NA*preftalent2
prefrichparents2 ~~ NA*prefrichparents2
prefcontact2 ~~ NA*prefcontact2


# Covarianzas entre errores longitudinales

perceffort1 ~~ perceffort2
perctalent1 ~~ perctalent2
percrichparents1 ~~ percrichparents2
perccontact1 ~~ perccontact2
prefeffort1 ~~ prefeffort2
preftalent1 ~~ preftalent2
prefrichparents1 ~~ prefrichparents2
prefcontact1 ~~ prefcontact2
')

fit_thresholdinv <- cfa(thresholdinv_model_smt, data = db_invariance,
                        ordered = c("perceffort1", "perctalent1", "perceffort2", "perctalent2",
                                    "percrichparents1", "perccontact1", "percrichparents2", "perccontact2",
                                    "prefeffort1", "preftalent1", "prefeffort2", "preftalent2",
                                    "prefrichparents1", "prefcontact1", "prefrichparents2", "prefcontact2"),
                        parameterization = "theta",
                        estimator = "WLSMV")
```

```{r}
#| label: strict-model
#| tbl-cap: "Longitudinal Invariance Strict Model"
#| results: asis
#| echo: false
#| warning: false

uniquenessinv_model_smt <- ('

# Cargas factoriales (invarianza débil)

percmerit1  =~ 1*perceffort1 + pe_loading*perctalent1
percmerit2  =~ 1*perceffort2 + pe_loading*perctalent2

percnmerit1 =~ 1*percrichparents1 + prc_loading*perccontact1
percnmerit2 =~ 1*percrichparents2 + prc_loading*perccontact2

prefmerit1  =~ 1*prefeffort1 + pfe_loading*preftalent1
prefmerit2  =~ 1*prefeffort2 + pfe_loading*preftalent2

prefnmerit1 =~ 1*prefrichparents1 + pfrc_loading*prefcontact1
prefnmerit2 =~ 1*prefrichparents2 + pfrc_loading*prefcontact2


# Varianzas y covarianzas de factores latentes

percmerit1  ~~ percmerit1 + percmerit2
percmerit2  ~~ percmerit2
percnmerit1 ~~ percnmerit1 + percnmerit2
percnmerit2 ~~ percnmerit2
prefmerit1  ~~ prefmerit1 + prefmerit2
prefmerit2  ~~ prefmerit2
prefnmerit1 ~~ prefnmerit1 + prefnmerit2
prefnmerit2 ~~ prefnmerit2


# Medias latentes

percmerit1  ~ 0*1
percmerit2  ~ 1
percnmerit1 ~ 0*1
percnmerit2 ~ 1
prefmerit1  ~ 0*1
prefmerit2  ~ 1
prefnmerit1 ~ 0*1
prefnmerit2 ~ 1


# Umbrales (invarianza fuerte)

perceffort1     | pe_t1*t1 + pe_t2*t2
perceffort2     | pe_t1*t1 + pe_t2*t2
perctalent1     | pt_t1*t1 + pt_t2*t2
perctalent2     | pt_t1*t1 + pt_t2*t2
percrichparents1| prp_t1*t1 + prp_t2*t2
percrichparents2| prp_t1*t1 + prp_t2*t2
perccontact1    | pc_t1*t1 + pc_t2*t2
perccontact2    | pc_t1*t1 + pc_t2*t2
prefeffort1     | pre_t1*t1 + pre_t2*t2
prefeffort2     | pre_t1*t1 + pre_t2*t2
preftalent1     | prt_t1*t1 + prt_t2*t2
preftalent2     | prt_t1*t1 + prt_t2*t2
prefrichparents1| pfrp_t1*t1 + pfrp_t2*t2
prefrichparents2| pfrp_t1*t1 + pfrp_t2*t2
prefcontact1    | pfc_t1*t1 + pfc_t2*t2
prefcontact2    | pfc_t1*t1 + pfc_t2*t2


# Interceptos fijos

perceffort1 + perctalent1 ~ 0*1
perceffort2 + perctalent2 ~ 0*1
percrichparents1 + perccontact1 ~ 0*1
percrichparents2 + perccontact2 ~ 0*1
prefeffort1 + preftalent1 ~ 0*1
prefeffort2 + preftalent2 ~ 0*1
prefrichparents1 + prefcontact1 ~ 0*1
prefrichparents2 + prefcontact2 ~ 0*1


# Varianzas únicas (invarianza estricta)
# Fijadas a 1 en todas las olas

perceffort1 ~~ 1*perceffort1
perceffort2 ~~ 1*perceffort2
perctalent1 ~~ 1*perctalent1
perctalent2 ~~ 1*perctalent2

percrichparents1 ~~ 1*percrichparents1
percrichparents2 ~~ 1*percrichparents2
perccontact1 ~~ 1*perccontact1
perccontact2 ~~ 1*perccontact2

prefeffort1 ~~ 1*prefeffort1
prefeffort2 ~~ 1*prefeffort2
preftalent1 ~~ 1*preftalent1
preftalent2 ~~ 1*preftalent2

prefrichparents1 ~~ 1*prefrichparents1
prefrichparents2 ~~ 1*prefrichparents2
prefcontact1 ~~ 1*prefcontact1
prefcontact2 ~~ 1*prefcontact2


# Covarianzas entre errores (mismo ítem, diferente ola)

perceffort1 ~~ perceffort2
perctalent1 ~~ perctalent2
percrichparents1 ~~ percrichparents2
perccontact1 ~~ perccontact2
prefeffort1 ~~ prefeffort2
preftalent1 ~~ preftalent2
prefrichparents1 ~~ prefrichparents2
prefcontact1 ~~ prefcontact2
')

fit_uniquenessinv <- cfa(uniquenessinv_model_smt, data = db_invariance,
                         ordered = c("perceffort1", "perctalent1", "perceffort2", "perctalent2",
                                     "percrichparents1", "perccontact1", "percrichparents2", "perccontact2",
                                     "prefeffort1", "preftalent1", "prefeffort2", "preftalent2",
                                     "prefrichparents1", "prefcontact1", "prefrichparents2", "prefcontact2"),
                         parameterization = "theta",
                         estimator = "WLSMV")
```

```{r}
#| label: fit-comp
#| tbl-cap: "Longitudinal Invariance Test Results 3"
#| results: asis
#| echo: false
#| warning: false

# Compare fit

an1 <- anova(fit_baseline, fit_loadinginv)
an2 <- anova(fit_loadinginv, fit_thresholdinv)
an3 <- anova(fit_thresholdinv, fit_uniquenessinv)

tab01 <- bind_rows(
  as_tibble(an1)[2,],
  as_tibble(an2)[2,],
  as_tibble(an3)[2,]
) %>%
  select("Chisq", "Df", chisq_diff = `Chisq diff`, df_diff = `Df diff`, pvalue = `Pr(>Chisq)`) %>%
  mutate(
    stars = stars.pval(pvalue),
    chisqt = paste0(round(Chisq, 2), " (", Df, ")"),
    decision = ifelse(pvalue > 0.05, "Accept", "Reject"),
    model = c("Weak", "Strong", "Strict")
  ) %>%
  bind_rows(
    tibble(Chisq = an1$Chisq[1], Df = an1$Df[1], chisq_diff = NA, df_diff = NA,
           pvalue = NA, stars = "", chisqt = paste0(round(an1$Chisq[1], 2), " (", an1$Df[1], ")"),
           decision = "Reference", model = "Configural")
  ) %>%
  select(model, chisqt, chisq_diff, df_diff, pvalue, stars, decision) %>%
  mutate(model = factor(model, levels = c("Configural", "Weak", "Strong", "Strict"))) %>%
  arrange(model)


fit.meas <- dplyr::bind_rows(lavaan::fitmeasures(fit_baseline, output ="matrix")[c("chisq","df","cfi","rmsea","rmsea.ci.lower","rmsea.ci.upper"),],
                             lavaan::fitmeasures(fit_loadinginv,  output ="matrix")[c("chisq","df","cfi","rmsea","rmsea.ci.lower","rmsea.ci.upper"),],
                             lavaan::fitmeasures(fit_thresholdinv, output ="matrix")[c("chisq","df","cfi","rmsea","rmsea.ci.lower","rmsea.ci.upper"),],
                             lavaan::fitmeasures(fit_uniquenessinv, output ="matrix")[c("chisq","df","cfi","rmsea","rmsea.ci.lower","rmsea.ci.upper"),])

fit.meas <- fit.meas %>%
  dplyr::mutate(diff.chi2 = chisq    - lag(chisq,default = dplyr::first(chisq)),
                diff.df   = df       - lag(df,   default = dplyr::first(df)),
                diff.cfi  = cfi      - lag(cfi,  default = dplyr::first(cfi)),
                diff.rmsea   = rmsea - lag(rmsea,default = dplyr::first(rmsea))) %>%
  round(3) %>%
  dplyr::mutate(rmsea.ci=paste0(rmsea," \n ", "(",rmsea.ci.lower,"-",rmsea.ci.upper,")"))

tab.inv <- dplyr::bind_cols(tab01,fit.meas) %>%
  dplyr::select(model,chisqt,cfi,rmsea.ci,diff.chi2,diff.df,diff.cfi,diff.rmsea,stars,decision) %>%
  dplyr::mutate(diff.chi2=paste0(diff.chi2," (",diff.df,") ",stars)) %>%
  dplyr::select(model,chisqt,cfi,rmsea.ci,diff.chi2,diff.cfi,diff.rmsea,decision)


col.nam <- c("Model","&chi;^2 (df)","CFI","RMSEA (90 CI)",
             "&Delta; &chi;^2 (&Delta; df)","&Delta; CFI","&Delta; RMSEA","Decision")

tab.inv %>% 
  kableExtra::kable(format = "html",
                    align = "c",
                    booktabs = T,
                    escape = F,
                    caption = NULL,
                    col.names = col.nam) %>%
  kableExtra::kable_styling(full_width = T,
                            latex_options = "hold_position",
                            bootstrap_options=c("striped", "bordered", "condensed"),
                            font_size = 23) %>%
  kableExtra::column_spec(c(1,8), width = "3.5cm") %>% 
  kableExtra::column_spec(2:7, width = "4cm") %>% 
  kableExtra::column_spec(4, width = "5cm")
```

A series of nested confirmatory factor analysis (CFA) models was estimated to assess the longitudinal measurement invariance of the constructs across different waves of the study. The evaluation began with the configural model, which allows all parameters (loadings, intercepts, and residuals) to vary freely across time. This model served as the baseline for subsequent comparisons and demonstrated good fit to the data, with χ²(68) = 117.7, a Comparative Fit Index (CFI) of 0.991, and a Root Mean Square Error of Approximation (RMSEA) of 0.035, with a 90% confidence interval ranging from 0.024 to 0.046. These indices suggest that the same factorial structure is replicated across time points, meaning that the general configuration of items and latent variables remains stable.

Subsequently, the weak invariance model was tested by constraining the factor loadings to be equal across waves. This model also showed excellent fit: χ²(72) = 122.51, CFI = 0.990, RMSEA = 0.034 (90% CI: 0.024–0.045). The comparison with the configural model revealed a negligible and non-significant change in model fit, with Δχ²(4) = 4.809, ΔCFI = 0.000, and ΔRMSEA = -0.001. These results support the assumption of weak invariance, indicating that the strength of the relationship between items and latent constructs is consistent over time.

The strong invariance model introduced additional constraints by setting item intercepts equal across time points, in addition to the equality of factor loadings. This model yielded a χ²(80) = 128.53, CFI = 0.991, and RMSEA = 0.032 (90% CI: 0.021–0.042). The changes in fit statistics relative to the weak model were Δχ²(8) = 6.02, ΔCFI = 0.000, and ΔRMSEA = -0.002. These minimal differences suggest that the additional constraints did not significantly impair the model’s fit. Therefore, strong invariance is supported, meaning that individuals with the same latent trait level are expected to have the same observed item scores across waves.

Finally, the strict invariance model imposed equality constraints on residual variances in addition to the loadings and intercepts. This model also showed good fit: χ²(84) = 130.17, CFI = 0.991, RMSEA = 0.031 (90% CI: 0.020–0.040). The comparison with the strong model revealed a Δχ²(4) = 1.635, ΔCFI = 0.000, and ΔRMSEA = -0.002. These changes are minimal and fall well within recommended thresholds for model comparison, indicating that strict invariance is also supported.

In summary, the results support full longitudinal measurement invariance—configural, weak, strong, and strict—suggesting that the constructs are measured equivalently across time. This justifies meaningful comparisons of latent means, variances, and covariances over time, and provides strong evidence of the temporal stability and psychometric robustness of the measurement model. However, it remains important to explore possible sources of misfit or localized non-invariance that could inform refinements to further improve the model.

# Possible Error Sources

```{r}
#| label: sources-weak
#| tbl-cap: "Longitudinal Invariance Sources"
#| results: asis
#| echo: false
#| warning: false


#Run lavTestScore on strong invariance model
score_result <- lavTestScore(fit_loadinginv)

#Extract univariate score test results
score_df <- score_result$uni

#Create readable table with evaluation

score_table <- score_df %>%
  mutate(
    restriction = paste(lhs, op, rhs),
    evaluation = case_when(
      p.value >= 0.05 ~ "No violation",
      p.value >= 0.01 ~ "Possible violation",
      TRUE ~ "Clear violation"
    )
  ) %>%
  select(restriction, X2, df, p.value, evaluation)

col.nam <- c("Restriction Tested", "Chi-square", "df", "p-value", "Evaluation")

score_table %>%
  kable(format = "html",
        align = "c",
        booktabs = TRUE,
        escape = FALSE,
        caption = "Assessment of Metric Invariance Based on Score Test (Weak Invariance Model)",
        col.names = col.nam,
        digits = 3) %>%
  kable_styling(
    full_width = TRUE,
    bootstrap_options = c("striped", "bordered", "condensed"),
    latex_options = "hold_position",
    font_size = 18
  ) %>%
  column_spec(1, width = "5cm") %>%
  column_spec(2:4, width = "3.5cm") %>%
  column_spec(5, width = "4cm")
```

The results of the score test for the weak invariance model, which evaluates the equality of factor loadings across time points, indicate that all tested parameter constraints are supported by the data. Specifically, none of the tested restrictions—such as the equality between parameters `.p2.` and `.p4.`, `.p6.` and `.p8.`, `.p10.` and `.p12.`, and `.p14.` and `.p16.`—yielded statistically significant chi-square values. All p-values were well above the conventional threshold of 0.05, suggesting that constraining these loadings to be equal does not lead to a significant deterioration in model fit.

These findings provide strong empirical support for metric (or weak) invariance, meaning that the strength of the relationship between the latent factors and their observed indicators is stable across waves. This level of invariance is a necessary condition for making valid comparisons of structural relationships—such as regression paths or covariances—between latent variables over time. The absence of any violation in the score test results implies that the measurement model functions consistently across the different time points examined.

```{r}
#| label: sources-strog
#| tbl-cap: "Possible Violations of Invariance"
#| results: asis
#| echo: false
#| warning: false

#Run lavTestScore on strong invariance model
score_result <- lavTestScore(fit_thresholdinv)

#Extract univariate score test results
score_df <- score_result$uni

#Create readable table with evaluation

score_table <- score_df %>%
  mutate(
    restriction = paste(lhs, op, rhs),
    evaluation = case_when(
      p.value >= 0.05 ~ "No violation",
      p.value >= 0.01 ~ "Possible violation",
      TRUE ~ "Clear violation"
    )
  ) %>%
  select(restriction, X2, df, p.value, evaluation)

col.nam <- c("Restriction Tested", "Chi-square", "df", "p-value", "Evaluation")

score_table %>%
  kable(format = "html",
        align = "c",
        booktabs = TRUE,
        escape = FALSE,
        caption = "Assessment of Metric Invariance Based on Score Test (Strong Invariance Model)",
        col.names = col.nam,
        digits = 3) %>%
  kable_styling(
    full_width = TRUE,
    bootstrap_options = c("striped", "bordered", "condensed"),
    latex_options = "hold_position",
    font_size = 18
  ) %>%
  column_spec(1, width = "5cm") %>%
  column_spec(2:4, width = "3.5cm") %>%
  column_spec(5, width = "4cm")
```

The table presents the results of score tests used to evaluate metric invariance under the **strict invariance model**, which imposes the strongest set of constraints in longitudinal measurement: equal factor loadings, intercepts, and residual variances across waves. Each row in the table corresponds to a test of the equality between two specific parameters across time points. The chi-square statistic, degrees of freedom (df), and p-value indicate whether that equality constraint significantly worsens model fit, which would suggest a violation of invariance.

In this case, all tested restrictions yield **non-significant** p-values, with none falling below the conventional alpha threshold of 0.05. For example, the constraint `.p2. == .p4.` produces a chi-square of 0.108 and a p-value of 0.742, strongly suggesting that the two parameters are statistically indistinguishable and thus invariant. Other constraints, such as `.p14. == .p16.` (χ² = 0.060, p = 0.806) and `.p66. == .p68.` (χ² = 0.006, p = 0.937), similarly indicate no violation of invariance.

A few parameters show slightly elevated chi-square values and marginal p-values—for instance, `.p49. == .p51.` yields a chi-square of 3.414 and a p-value of 0.065. While still non-significant, this result is relatively close to the threshold, and the associated parameters might merit further investigation for potential partial invariance or item-level idiosyncrasies.

Overall, the evidence suggests that the model meets the criteria for strict invariance. That is, there is no statistical indication that any of the tested loadings, intercepts, or residuals differ meaningfully across waves. This supports the conclusion that the measurement properties of the constructs remain stable over time. Still, researchers should remain cautious and consider examining parameters close to significance thresholds, exploring possible sources of measurement error, or testing for partial invariance if theoretical or substantive justifications arise.

```{r}
#| label: sources-strict
#| tbl-cap: "Possible Violations of Invariance"
#| results: asis
#| echo: false
#| warning: false

#Run lavTestScore on strong invariance model
score_result <- lavTestScore(fit_uniquenessinv)

#Extract univariate score test results
score_df <- score_result$uni

#Create readable table with evaluation

score_table <- score_df %>%
  mutate(
    restriction = paste(lhs, op, rhs),
    evaluation = case_when(
      p.value >= 0.05 ~ "No violation",
      p.value >= 0.01 ~ "Possible violation",
      TRUE ~ "Clear violation"
    )
  ) %>%
  select(restriction, X2, df, p.value, evaluation)

col.nam <- c("Restriction Tested", "Chi-square", "df", "p-value", "Evaluation")

score_table %>%
  kable(format = "html",
        align = "c",
        booktabs = TRUE,
        escape = FALSE,
        caption = "Assessment of Metric Invariance Based on Score Test (Strict Invariance Model)",
        col.names = col.nam,
        digits = 3) %>%
  kable_styling(
    full_width = TRUE,
    bootstrap_options = c("striped", "bordered", "condensed"),
    latex_options = "hold_position",
    font_size = 18
  ) %>%
  column_spec(1, width = "5cm") %>%
  column_spec(2:4, width = "3.5cm") %>%
  column_spec(5, width = "4cm")
```

The results of score tests used to evaluate metric invariance under the **strict invariance model**, which imposes the strongest set of constraints in longitudinal measurement, all tested restrictions yield **non-significant** p-values, with none falling below the conventional alpha threshold of 0.05. For example, the constraint `.p2. == .p4.` produces a chi-square of 0.108 and a p-value of 0.742, strongly suggesting that the two parameters are statistically indistinguishable and thus invariant. Other constraints, such as `.p14. == .p16.` (χ² = 0.060, p = 0.806) and `.p66. == .p68.` (χ² = 0.006, p = 0.937), similarly indicate no violation of invariance.

A few parameters show slightly elevated chi-square values and marginal p-values—for instance, `.p49. == .p51.` yields a chi-square of 3.414 and a p-value of 0.065. While still non-significant, this result is relatively close to the threshold, and the associated parameters might merit further investigation for potential partial invariance or item-level idiosyncrasies.

Overall, the evidence suggests that the model meets the criteria for strict invariance. That is, there is no statistical indication that any of the tested loadings, intercepts, or residuals differ meaningfully across waves. This supports the conclusion that the measurement properties of the constructs remain stable over time. Still, researchers should remain cautious and consider examining parameters close to significance thresholds, exploring possible sources of measurement error, or testing for partial invariance if theoretical or substantive justifications arise.
